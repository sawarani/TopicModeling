{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9b41e7-a5bf-40e7-b4c5-56ffa647a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baa6bd0-90b7-4849-b323-fbeb378d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import pymorphy2\n",
    "from gensim.models import Phrases\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb80711b-53cf-4a76-8209-11507340d76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do NOT split intra-hyphen words (spaCy)\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search, \\\n",
    "                     suffix_search=suffix_re.search, \\\n",
    "                     infix_finditer=infix_re.finditer, \\\n",
    "                     token_match=None)\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('ru_core_news_lg')\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "nlp.add_pipe('merge_entities') # allows for merging with _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe41e00c-bdd0-42a5-8c09-a6a48914fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab06dc8-f107-44c7-98c7-21d4e02310e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_texts_to_json(json, num, texts):\n",
    "    for i in range(num):\n",
    "        text = json['Author'][i]['Full text']\n",
    "        no_hyphen = re.sub('[\\xad…]', '', text) # soft hyphen character          \n",
    "        no_space = re.sub('[\\xa0]', ' ', no_hyphen) # no-break space\n",
    "        texts.append(no_space)\n",
    "\n",
    "def get_lemmas(texts):\n",
    "    return [[morph.normal_forms(word)[0] if '_' not in word \\\n",
    "             else '_'.join(morph.normal_forms(i)[0] for i in word.split('_')) \\\n",
    "             for word in text] for text in texts]\n",
    "\n",
    "def get_named_ents(texts):\n",
    "    data = []\n",
    "    for text in texts:\n",
    "        data.append([str(word) if not word.ent_type_ else str(word).replace(' ', '_') \\\n",
    "                     for word in nlp(text)])\n",
    "    return data\n",
    "\n",
    "def get_ngrams(texts):\n",
    "    ngram = Phrases(texts, min_count=10, threshold=100)\n",
    "    return ngram[texts]\n",
    "\n",
    "def get_nouns(texts):\n",
    "    return [[word for word in text \\\n",
    "             if morph.parse(word)[0].tag.POS == 'NOUN'] \\\n",
    "            for text in texts]\n",
    "\n",
    "def get_nouns_adj(texts):\n",
    "    return [[word for word in text \\\n",
    "             if morph.parse(word)[0].tag.POS == 'NOUN' \\\n",
    "             or morph.parse(word)[0].tag.POS == 'ADJF'] \\\n",
    "            for text in texts]\n",
    "\n",
    "def clean_texts(texts):\n",
    "    punct = string.punctuation # !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "    punct += '—“”«»<>…...°1234567890'\n",
    "    remove = punct.replace('_', '').replace('-', '') # do not remove\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "    data = [[re.sub(pattern, '', word).strip('_-') for word in text] \\\n",
    "            for text in texts]\n",
    "    return [[word.replace('__', '_') for word in text if word] \\\n",
    "            for text in data]\n",
    "\n",
    "def remove_stopwords(texts, words):\n",
    "    with open('swl_optimum.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().split('\\n') \n",
    "        stopwords.extend(words)\n",
    "        data = [[word for word in text if re.sub('[Ёё]', 'е', word) \\\n",
    "                 not in stopwords or word not in stopwords] for text in texts]\n",
    "        # ignore words that contain only latin characters\n",
    "        return [[word for word in text if re.search(r'[^a-zA-Z]+', word)] \\\n",
    "                for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c62a78-3a6c-4743-be6c-6af1c657bf99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirpath = os.getcwd() + '\\\\corpus\\\\'\n",
    "\n",
    "js_docs, meta = [], []\n",
    "\n",
    "for file in os.listdir(dirpath):\n",
    "    if file.startswith('__'):\n",
    "        with open(dirpath+file, 'r') as f:\n",
    "            js = json.load(f)\n",
    "            # authors' names to remove later\n",
    "            meta.append(js['Author'][0]['Author'].replace(' ', '_'))\n",
    "            \n",
    "            if len(js['Author']) > 100: # no more than 100 texts per author\n",
    "                num = 100\n",
    "            else: \n",
    "                num = len(js['Author'])\n",
    "            add_texts_to_json(js, num, js_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261c3657-5d90-446b-a16f-7041681f91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Evaluate the size of the corpus before preprocessing\n",
    "tokens = [word_tokenize(doc) for doc in js_docs]\n",
    "tokens_clean = clean_texts(tokens)\n",
    "data = [[word for word in text if word] for text in tokens_clean]\n",
    "corpus_size = sum([len(token) for token in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3eb3d5-42a9-49b8-b27d-a39c65c96c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Named entity recognition + tokenization\n",
    "\n",
    "texts_named_ents = get_named_ents(js_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df771600-40f0-4b45-b8bd-c639ad71f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove punctuation\n",
    "\n",
    "texts_no_punct = clean_texts(texts_named_ents)\n",
    "\n",
    "# Save\n",
    "with open('tokens.json', 'w') as f:\n",
    "    json.dump(texts_no_punct, f, indent=4)\n",
    "# Load\n",
    "with open(f'tokens.json', 'r') as f:\n",
    "    texts_no_punct = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4b2496-4405-4c74-9076-2259780dbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lemmatize\n",
    "\n",
    "lemmas = get_lemmas(texts_no_punct)\n",
    "\n",
    "# Save\n",
    "with open(f'lemmas.json', 'w') as f:\n",
    "    json.dump(lemmas, f, indent=4)\n",
    "# Load\n",
    "with open(f'lemmas.json', 'r') as f:\n",
    "    lemmas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997350ce-a391-47d9-aa19-d8600599db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove lemmas with length 2 and less\n",
    "\n",
    "lemmas_no_short = [[word for word in text if len(word) > 2] \\\n",
    "                   for text in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c54630-f54c-4b1f-a33f-bd1f4c1e0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Remove stopwords\n",
    "\n",
    "lemmas_no_sw = remove_stopwords(lemmas_no_short, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed90c114-593f-49a2-8b75-350f3dcb0fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Retrieve collocations/n-grams\n",
    "\n",
    "texts_ngrams = get_ngrams(lemmas_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3551fe-699f-481e-9ee4-f4b8a84ae78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Keep nouns only\n",
    "\n",
    "nouns = get_nouns(texts_ngrams)\n",
    "\n",
    "# Save\n",
    "with open(f'nouns.json', 'w') as f:\n",
    "    json.dump(nouns, f, indent=4)\n",
    "# Load\n",
    "with open(f'nouns.json', 'r') as f:\n",
    "    nouns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2577e4d2-e715-4eb6-8227-a38295a30197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 (alt). Keep nouns and adjectives only\n",
    "\n",
    "nouns_adj = get_nouns_adj(texts_ngrams)\n",
    "\n",
    "# Save\n",
    "with open(f'nouns_adj.json', 'w') as f:\n",
    "    json.dump(nouns_adj, f, indent=4)\n",
    "# Load\n",
    "with open(f'nouns_adj.json', 'r') as f:\n",
    "    nouns_adj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19ef6bb-58ca-4ca4-ad91-05320faffab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1260\n",
      "\n",
      "Corpus size\n",
      "\tBefore preprocessing: 1,585,992\n",
      "\tAfter tokenization and NER: 1,952,298\n",
      "\tAfter lemmatization: 1,570,105\n",
      "\tAfter stopwords removal: 987,524\n",
      "\tAfter n-gram extraction: 955,746\n",
      "\tAfter bad POS removal (nouns): 513,409\n",
      "\tAfter bad POS removal (nouns and adj): 690,221\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "\n",
    "print(f'Number of documents: {len(js_docs)}')\n",
    "\n",
    "print(f'\\nCorpus size\\n\\tBefore preprocessing: {corpus_size:,}' +\n",
    "      f'\\n\\tAfter tokenization and NER: {sum([len(doc) for doc in texts_named_ents]):,}' +\n",
    "      f'\\n\\tAfter lemmatization: {sum([len(doc) for doc in lemmas]):,}' +\n",
    "      f'\\n\\tAfter stopwords removal: {sum([len(doc) for doc in lemmas_no_sw]):,}' +\n",
    "      f'\\n\\tAfter n-gram extraction: {sum([len(doc) for doc in texts_ngrams]):,}' +\n",
    "      f'\\n\\tAfter bad POS removal (nouns): {sum([len(doc) for doc in nouns]):,}' +\n",
    "      f'\\n\\tAfter bad POS removal (nouns and adj): {sum([len(doc) for doc in nouns_adj]):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a73db204-d603-4b35-9234-0bed2e789112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Козловский:\n",
      "\tNumber of articles: 43\n",
      "\tTotal number of words: 38259\n",
      "\tAverage number of words in an article: 890\n",
      "\n",
      "Александр Марков:\n",
      "\tNumber of articles: 100\n",
      "\tTotal number of words: 163181\n",
      "\tAverage number of words in an article: 1632\n",
      "\n",
      "Александр Сергеев:\n",
      "\tNumber of articles: 84\n",
      "\tTotal number of words: 47791\n",
      "\tAverage number of words in an article: 569\n",
      "\n",
      "Алексей Гиляров:\n",
      "\tNumber of articles: 100\n",
      "\tTotal number of words: 83601\n",
      "\tAverage number of words in an article: 836\n",
      "\n",
      "Алексей Левин:\n",
      "\tNumber of articles: 85\n",
      "\tTotal number of words: 178758\n",
      "\tAverage number of words in an article: 2103\n",
      "\n",
      "Алексей Опаев:\n",
      "\tNumber of articles: 61\n",
      "\tTotal number of words: 60229\n",
      "\tAverage number of words in an article: 987\n",
      "\n",
      "Аркадий Курамшин:\n",
      "\tNumber of articles: 42\n",
      "\tTotal number of words: 44658\n",
      "\tAverage number of words in an article: 1063\n",
      "\n",
      "Варвара Веденина:\n",
      "\tNumber of articles: 67\n",
      "\tTotal number of words: 64950\n",
      "\tAverage number of words in an article: 969\n",
      "\n",
      "Вера Башмакова:\n",
      "\tNumber of articles: 55\n",
      "\tTotal number of words: 52421\n",
      "\tAverage number of words in an article: 953\n",
      "\n",
      "Владислав Стрекопытов:\n",
      "\tNumber of articles: 98\n",
      "\tTotal number of words: 109166\n",
      "\tAverage number of words in an article: 1114\n",
      "\n",
      "Вячеслав Калинин:\n",
      "\tNumber of articles: 54\n",
      "\tTotal number of words: 56397\n",
      "\tAverage number of words in an article: 1044\n",
      "\n",
      "Елена Наймарк:\n",
      "\tNumber of articles: 100\n",
      "\tTotal number of words: 102996\n",
      "\tAverage number of words in an article: 1030\n",
      "\n",
      "Игорь Иванов:\n",
      "\tNumber of articles: 100\n",
      "\tTotal number of words: 167535\n",
      "\tAverage number of words in an article: 1675\n",
      "\n",
      "Сергей Ястребов:\n",
      "\tNumber of articles: 85\n",
      "\tTotal number of words: 192578\n",
      "\tAverage number of words in an article: 2266\n",
      "\n",
      "Татьяна Романовская:\n",
      "\tNumber of articles: 42\n",
      "\tTotal number of words: 76565\n",
      "\tAverage number of words in an article: 1823\n",
      "\n",
      "Юлия Кондратенко:\n",
      "\tNumber of articles: 56\n",
      "\tTotal number of words: 51499\n",
      "\tAverage number of words in an article: 920\n",
      "\n",
      "Юрий Ерин:\n",
      "\tNumber of articles: 88\n",
      "\tTotal number of words: 95408\n",
      "\tAverage number of words in an article: 1084\n",
      "\n",
      "Total number of authors: 17\n",
      "Total number of articles: 1260\n",
      "Corpus size (before preprocessing): 1,585,992\n"
     ]
    }
   ],
   "source": [
    "# More statistics\n",
    "\n",
    "def add_to_docs(file):\n",
    "    docs = []\n",
    "    with open(dirpath+file, 'r') as f:\n",
    "        js = json.load(f)\n",
    "        name = js['Author'][0]['Author']\n",
    "        if len(js['Author']) > 100: # no more than 100 texts per author\n",
    "            num = 100\n",
    "        else:\n",
    "            num = len(js['Author'])\n",
    "        add_texts_to_json(js, num, docs)\n",
    "        return docs, name, num\n",
    "            \n",
    "def count_tokens(texts):\n",
    "    tokens = [word_tokenize(text) for text in texts]\n",
    "    tokens_clean = clean_texts(tokens)\n",
    "    data = [[word for word in text if word] for text in tokens_clean]\n",
    "    count = sum([len(token) for token in data])\n",
    "    return count, round(count/len(texts))\n",
    "\n",
    "\n",
    "n, m, k = 0, 0, 0\n",
    "for file in os.listdir(dirpath):\n",
    "    if file.endswith('EL.json'):\n",
    "        n += 1\n",
    "        texts, name, num_art = add_to_docs(file)\n",
    "        count, avg_count = count_tokens(texts)\n",
    "        k += count\n",
    "        m += num_art\n",
    "        print(f'{name}:\\n\\tNumber of articles: {num_art}' + \n",
    "              f'\\n\\tTotal number of words: {count}' + \n",
    "              f'\\n\\tAverage number of words in an article: {avg_count}\\n')\n",
    "\n",
    "print(f'Total number of authors: {n}' + \n",
    "      f'\\nTotal number of articles: {m}' + \n",
    "      f'\\nCorpus size (before preprocessing): {k:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570a454-1255-470c-ba99-4ecfbd127dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
