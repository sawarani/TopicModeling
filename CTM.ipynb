{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd384b3-3688-4fc0-945a-695f6cb32d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('0-texts-ctm.json', 'r') as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c9388-6b34-419e-a251-d46e9fdfb25d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('0-nouns_adj.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "docs_flat = [item for doc in documents for item in doc] # flatten\n",
    "c = Counter(docs_flat)\n",
    "most_common = [key for key, _ in c.most_common()[:25]] # 25 most common words\n",
    "least_common = [key for key, value in c.most_common() if value == 1] # occur only once\n",
    "\n",
    "stoplist = most_common + least_common + ['нью', 'по']\n",
    "\n",
    "docs = [[word for word in doc if len(word) > 2 and word not in stoplist] \\\n",
    "        for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10dc8fd2-4444-4ad4-bc42-dfba9fd40bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "\n",
    "sp = WhiteSpacePreprocessingStopwords(docs, 'russian')\n",
    "preprocessed_documents, unpreprocessed_documents, vocab = sp.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461b2727-4acd-4870-b485-f663fbbe7048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\satan/.cache\\torch\\sentence_transformers\\DeepPavlov_rubert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\satan/.cache\\torch\\sentence_transformers\\DeepPavlov_rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c5ed121b384610bbe60fd87f623de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zero-Shot\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
    "\n",
    "# Initialize a contextualized model\n",
    "qt_z = TopicModelDataPreparation('DeepPavlov/rubert-base-cased') # BERT model\n",
    "\n",
    "# Create the training set\n",
    "training_dataset_z = qt_z.fit(text_for_contextual=unpreprocessed_documents, \\\n",
    "                              text_for_bow=preprocessed_documents)\n",
    "\n",
    "# Train the model\n",
    "num_topics = 10\n",
    "ctm_zero = ZeroShotTM(bow_size=len(qt_z.vocab), contextual_size=768, \\\n",
    "                      n_components=num_topics, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d832d7f7-f925-44c7-b69e-696aa9d0b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [50/50]\t Seen Samples: [91200/91200]\tTrain Loss: 5303.0183747944075\tTime: 0:00:16.225645: : 50it [13:43, 16.48s/it]\n",
      "Sampling: [20/20]: : 20it [05:16, 15.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "ctm_zero.fit(training_dataset_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b384c281-2652-4af4-bdad-1ce51cc9a8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['возраст',\n",
       "              'источник',\n",
       "              'млн',\n",
       "              'авторов',\n",
       "              'содержание',\n",
       "              'современного',\n",
       "              'прошлом',\n",
       "              'возраста',\n",
       "              'углерода',\n",
       "              'видов',\n",
       "              'считают',\n",
       "              'морские',\n",
       "              'океана',\n",
       "              'привело',\n",
       "              'показало'],\n",
       "             1: ['кто',\n",
       "              'литературе',\n",
       "              'русские',\n",
       "              'власти',\n",
       "              'xviii',\n",
       "              'подробно',\n",
       "              'половине',\n",
       "              'своем',\n",
       "              'нами',\n",
       "              'языке',\n",
       "              'живет',\n",
       "              'настолько',\n",
       "              'начал',\n",
       "              'нас',\n",
       "              'автор'],\n",
       "             2: ['сам',\n",
       "              'литературе',\n",
       "              'xviii',\n",
       "              'такое',\n",
       "              'русские',\n",
       "              'язык',\n",
       "              'общества',\n",
       "              'начал',\n",
       "              'власти',\n",
       "              'подробно',\n",
       "              'нами',\n",
       "              'автор',\n",
       "              'половине',\n",
       "              'веке',\n",
       "              'свое'],\n",
       "             3: ['что',\n",
       "              'не',\n",
       "              'на',\n",
       "              'то',\n",
       "              'его',\n",
       "              'это',\n",
       "              'он',\n",
       "              'но',\n",
       "              'как',\n",
       "              'по',\n",
       "              'из',\n",
       "              'же',\n",
       "              'так',\n",
       "              'было',\n",
       "              'все'],\n",
       "             4: ['клеток',\n",
       "              'обсуждаемои',\n",
       "              'видов',\n",
       "              'опухоли',\n",
       "              'ученые',\n",
       "              'клетки',\n",
       "              'рис',\n",
       "              'мышеи',\n",
       "              'рака',\n",
       "              'рисунок',\n",
       "              'оказалось',\n",
       "              'самцов',\n",
       "              'статьи',\n",
       "              'самок',\n",
       "              'при'],\n",
       "             5: ['ученые',\n",
       "              'обсуждаемои',\n",
       "              'оказалось',\n",
       "              'клеток',\n",
       "              'самок',\n",
       "              'мышеи',\n",
       "              'видов',\n",
       "              'различия',\n",
       "              'опухоли',\n",
       "              'клетки',\n",
       "              'самцов',\n",
       "              'виды',\n",
       "              'сигналов',\n",
       "              'двух',\n",
       "              'источник'],\n",
       "             6: ['года',\n",
       "              'физике',\n",
       "              'на',\n",
       "              'рис',\n",
       "              'физики',\n",
       "              'элементарных',\n",
       "              'энергии',\n",
       "              'частиц',\n",
       "              'при',\n",
       "              'частицы',\n",
       "              'материи',\n",
       "              'бозона',\n",
       "              'детектора',\n",
       "              'по',\n",
       "              'теории'],\n",
       "             7: ['температуры',\n",
       "              'источник',\n",
       "              'возраст',\n",
       "              'содержание',\n",
       "              'вода',\n",
       "              'давление',\n",
       "              'поверхность',\n",
       "              'км',\n",
       "              'авторов',\n",
       "              'ученых',\n",
       "              'изменения',\n",
       "              'прошлом',\n",
       "              'процессов',\n",
       "              'воды',\n",
       "              'затмения'],\n",
       "             8: ['см',\n",
       "              'рис',\n",
       "              'генов',\n",
       "              'элементы',\n",
       "              'что',\n",
       "              'гены',\n",
       "              'животных',\n",
       "              'по',\n",
       "              'из',\n",
       "              'эволюции',\n",
       "              'современных',\n",
       "              'эукариот',\n",
       "              'of',\n",
       "              'на',\n",
       "              'не'],\n",
       "             9: ['что',\n",
       "              'то',\n",
       "              'не',\n",
       "              'это',\n",
       "              'как',\n",
       "              'но',\n",
       "              'на',\n",
       "              'мы',\n",
       "              'его',\n",
       "              'он',\n",
       "              'люди',\n",
       "              'мне',\n",
       "              'есть',\n",
       "              'меня',\n",
       "              'когда']})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctm_zero.get_topics(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3979f-2404-4216-8878-a75d8960cfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctm_zero.get_wordcloud(topic_id=4, n_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926bc6bd-c72c-4847-899f-00f9b4299a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\satan/.cache\\torch\\sentence_transformers\\Tatyana_rubert-base-cased-sentiment-new. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\satan/.cache\\torch\\sentence_transformers\\Tatyana_rubert-base-cased-sentiment-new were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515c5b961e624572afda3907a673f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combined TM\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "\n",
    "# Initialize a contextualized model\n",
    "qt_c = TopicModelDataPreparation(\"Tatyana/rubert-base-cased-sentiment-new\") # BERT model\n",
    "\n",
    "# Create the training set\n",
    "training_dataset_c = qt_c.fit(text_for_contextual=unpreprocessed_documents, \\\n",
    "                              text_for_bow=preprocessed_documents)\n",
    "\n",
    "# Train the model\n",
    "num_topics = 20\n",
    "ctm_comb = CombinedTM(bow_size=len(qt_c.vocab), contextual_size=768, \\\n",
    "                      n_components=num_topics, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b29a1032-5e3e-4537-b390-4b7349d956ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [50/50]\t Seen Samples: [69100/69100]\tTrain Loss: 3137.0402496382053\tTime: 0:00:16.800953: : 50it [14:06, 16.93s/it]\n",
      "Sampling: [20/20]: : 20it [05:33, 16.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "ctm_comb.fit(training_dataset_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d5811-90c9-441e-b4db-1c4f3bcb929c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(ctm_comb.get_topics(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad9fc9-ee58-476a-8794-0d23f073a5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
