{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6856df26-a93c-4dc3-8ce8-5799bcdf8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ssl\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73909d2c-1b93-457b-a4ee-302026ced429",
   "metadata": {},
   "source": [
    "### ЭЛЕМЕНТЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7babbeda-877a-4444-86f6-afc6231c38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the URL for each author\n",
    "\n",
    "link = 'https://elementy.ru/novosti_nauki'\n",
    "\n",
    "author_urls = []\n",
    "\n",
    "html = urlopen(link, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tags = soup('div', class_='sublink toggle_body minimize')[1].find_all('a')\n",
    "\n",
    "for tag in tags:\n",
    "    author_urls.append('https://elementy.ru' + tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410385cd-da16-4ad4-83a9-3640f5cd9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of pages for each author\n",
    "\n",
    "auth_pages = []\n",
    "\n",
    "for url in author_urls:\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        pages = soup('a', href=re.compile('.+?page=.+'))\n",
    "        num = int(pages[-2].get_text())\n",
    "    except:\n",
    "        num = 1\n",
    "    auth_pages.append((url, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d3fe55-bbdc-492d-a868-ee03f63c2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article URLs for each author\n",
    "\n",
    "names = []\n",
    "d = dict()\n",
    "\n",
    "for item in auth_pages:\n",
    "    url, pages = item[0], item[1]\n",
    "    \n",
    "    if pages > 2: # authors with at least 3 pages (~40 articles)\n",
    "        art, arts = dict(), list()\n",
    "        html = urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        name = re.search(r'(.+?)(?= •)', soup.find('title').get_text(' ', strip=True)).group()\n",
    "        names.append(name)\n",
    "\n",
    "        for i in range(int(pages)): # look at each page\n",
    "\n",
    "            html = urlopen(url + f'?page={i}', context=ctx).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            tags = soup('div', class_='clblock newslist')\n",
    "\n",
    "            for tag in tags:\n",
    "                hrefs = tag('a', class_='nohover', href=re.compile('/novosti_nauki/[0-9]+/.+'))\n",
    "                for href in hrefs:\n",
    "                    link = 'https://elementy.ru' + href['href']\n",
    "\n",
    "                    art = {'Article URL': link}\n",
    "                    arts.append(art)\n",
    "                    \n",
    "        d[name] = arts\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('elementy_authors.json', 'w') as f:\n",
    "    json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad47c6b6-5627-4e3a-a25e-e313bc52fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve article texts and metadata for chosen authors\n",
    "\n",
    "auth = dict()\n",
    "\n",
    "for name in names:\n",
    "    data = list()\n",
    "\n",
    "    with open('elementy_authors.json', 'r') as f:\n",
    "        js = json.load(f)\n",
    "        \n",
    "        for i in range(len(js[name])):\n",
    "            d = dict()\n",
    "            \n",
    "            url = js[name][i]['Article URL']\n",
    "            html = urlopen(url, context=ctx).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Metadata\n",
    "            meta = soup.find('div', class_='mb itemhead newslist')\n",
    "            title = meta.contents[0].get_text()\n",
    "            date = meta.find('span', class_='date').get_text()\n",
    "            topics = ', '.join([topic.get_text() for topic in meta('a')[1:-1]])\n",
    "\n",
    "            pattern = re.compile('Источник(и?)|См. также(:)?(\\s)?')\n",
    "            \n",
    "            # Full text\n",
    "            text = ''\n",
    "            tags = soup.find('div', class_='memo').contents\n",
    "            for tag in tags[:-2]: # ignore author's signature\n",
    "                # ignore illustrations, tables, etc.\n",
    "                if tag.name == 'p' or tag.name == 'ul':\n",
    "                    # ignore source materials\n",
    "                    if tag.get('class') and tag.get('class')[0] == 'small': continue\n",
    "                    if tag.find('b') and pattern.match(tag.find('b').get_text()): continue\n",
    "                    \n",
    "                    text += tag.get_text(' ', strip=True) + ' ' \n",
    "\n",
    "            if text:\n",
    "                d = {'Author': name, 'Title': title, 'Publication date': date, \\\n",
    "                    'Topics': topics, 'Article URL': url, 'Full text': text}\n",
    "                data.append(d)\n",
    "                \n",
    "        auth[name] = data\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('elementy_texts.json', 'w') as f:\n",
    "    json.dump(auth, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a72475a4e957f639a8707c38a651116e9f027e6c6985c08c8cef46a94d0a95f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
