{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6856df26-a93c-4dc3-8ce8-5799bcdf8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ssl\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73909d2c-1b93-457b-a4ee-302026ced429",
   "metadata": {},
   "source": [
    "### ЭЛЕМЕНТЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7babbeda-877a-4444-86f6-afc6231c38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the URL for each author\n",
    "\n",
    "link = 'https://elementy.ru/novosti_nauki'\n",
    "\n",
    "author_urls = []\n",
    "\n",
    "html = urlopen(link, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tags = soup('div', class_='sublink toggle_body minimize')[1].find_all('a')\n",
    "\n",
    "for tag in tags:\n",
    "    author_urls.append('https://elementy.ru' + tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410385cd-da16-4ad4-83a9-3640f5cd9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of pages for each author\n",
    "\n",
    "auth_pages = []\n",
    "\n",
    "for url in author_urls:\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        pages = soup('a', href=re.compile('.+?page=.+'))\n",
    "        num = int(pages[-2].get_text())\n",
    "    except:\n",
    "        num = 1\n",
    "    auth_pages.append((url, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d3fe55-bbdc-492d-a868-ee03f63c2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL and publication date for every single article and group them by author\n",
    "\n",
    "names, d = [], {}\n",
    "\n",
    "for item in auth_pages:\n",
    "    url, pages = item[0], item[1]\n",
    "    \n",
    "    article, articles = {}, []\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    name = re.search(r'(.+?)(?= •)', soup.find('title').get_text(' ', strip=True)).group()\n",
    "    names.append(name)\n",
    "\n",
    "    for i in range(int(pages)): # look at each page\n",
    "\n",
    "        html = urlopen(url + f'?page={i}', context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        tags = soup('div', class_='img_block32')\n",
    "\n",
    "        for tag in tags:\n",
    "            href = tag.find('a', class_='nohover', href=re.compile('/novosti_nauki/[0-9]+/.+'))\n",
    "            date = tag.find('span', class_='date').get_text()\n",
    "            if len(date) < 6: # for newer texts\n",
    "                date += '.2023'\n",
    "            link = 'https://elementy.ru' + href['href']\n",
    "\n",
    "            article = {'Article URL': link, 'Publication date': date}\n",
    "            articles.append(article)\n",
    "                \n",
    "    d[name] = articles\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('elementy_authors.json', 'w') as f:\n",
    "    json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad47c6b6-5627-4e3a-a25e-e313bc52fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Айк Акопян\n",
      "Валентин Анаников\n",
      "Ольга Баклицкая-Каменева\n",
      "Дарья Баранова\n",
      "Вера Башмакова\n",
      "Арсений Белосохов\n",
      "Александр Бердичевский\n",
      "Александр Березин\n",
      "Антон Бирюков\n",
      "Алексей Бондарев\n",
      "Максим Борисов\n",
      "Варвара Бусова\n",
      "Анастасия Вабищевич\n",
      "Ольга Вахрушева\n",
      "Варвара Веденина\n",
      "Александр Венедюхин\n",
      "Кирилл Власов\n",
      "Михаил Волович\n",
      "Эдуард Галоян\n",
      "Михаил Гарбузов\n",
      "Алексей Гиляров\n",
      "Ольга Гилярова\n",
      "Дмитрий Гиляров\n",
      "Сергей Глаголев\n",
      "Михаил Гопко\n",
      "Евгений Гордеев\n",
      "Анастасия Горелова\n",
      "Николай Горностаев\n",
      "Екатерина Грачева\n",
      "Владимир Гриньков\n",
      "Анна Гусева\n",
      "Дмитрий Дагаев\n",
      "Ира Демина\n",
      "Екатерина Диффинэ\n",
      "Татьяна Долгова\n",
      "Мария Елифёрова\n",
      "Юрий Ерин\n",
      "Анастасия Еськова\n",
      "Дмитрий Жарков\n",
      "Андрей Журавлёв\n",
      "Дмитрий Замолодчиков\n",
      "Никита Зеленков\n",
      "Денис Земледельцев\n",
      "Игорь Иванов\n",
      "Сергей Измалков\n",
      "Вячеслав Калинин\n",
      "Андрей Карпачевский\n",
      "Анна Каспарсон\n",
      "Павел Квартальнов\n",
      "Анастасия Кириллова\n",
      "Мария Кирсанова\n",
      "Дмитрий Кирюхин\n",
      "Галина Клинк\n",
      "Дмитрий Кнорре\n",
      "Александр Козловский\n",
      "Сергей Коленов\n",
      "Кирилл Колесников\n",
      "Людмила Колупаева\n",
      "Юлия Кондратенко\n",
      "Андрей Коньков\n",
      "Артём Коржиманов\n",
      "Ольга Кочина\n",
      "Юлия Краус\n",
      "Леонид Кузьмин\n",
      "Владимир Кукулин\n",
      "Георгий Куракин\n",
      "Аркадий Курамшин\n",
      "Виталий Кушниров\n",
      "Иван Лаврёнов\n",
      "Надежда Лапина\n",
      "Алексей Левин\n",
      "Дмитрий Леонтьев\n",
      "Андрей Логинов\n",
      "Полина Лосева\n",
      "Сергей Лысенков\n",
      "Лейла Мамирова\n",
      "Александр Марков\n",
      "Даниил Марков\n",
      "Александр Марфин\n",
      "Мария Медникова\n",
      "Елизавета Минина\n",
      "Александр Мироненко\n",
      "Юлия Михневич\n",
      "Вадим Мокиевский\n",
      "Григорий Молев\n",
      "Тарас Молотилин\n",
      "Антон Морковин\n",
      "Марат Мусин\n",
      "Вера Мухина\n",
      "Максим Нагорных\n",
      "Елена Наймарк\n",
      "Антон Нелихов\n",
      "Александра Нечаева\n",
      "Анна Новиковская\n",
      "Алексей Опаев\n",
      "Алексей Паевский\n",
      "Андрей Панкратов\n",
      "Анастасия Пашутова\n",
      "Петр Петров\n",
      "Александр Пиперски\n",
      "Константин Попадьин\n",
      "Сергей Попов\n",
      "Андрей Райгородский\n",
      "Роман Ракитов\n",
      "Жанна Резникова\n",
      "Татьяна Романовская\n",
      "Константин Рыбаков\n",
      "Анна Сабурова\n",
      "Александр Самардак\n",
      "Наталия Самойлова\n",
      "Вероника Самоцкая\n",
      "Иван Семенков\n",
      "Александр Сергеев\n",
      "Андрей Сидоренко\n",
      "Илья Скляр\n",
      "Виктория Скобеева\n",
      "Даниил Смирнов\n",
      "Павел Смирнов\n",
      "Олег Соколенко\n",
      "Дарья Спасская\n",
      "Михаил Столповский\n",
      "Владислав Стрекопытов\n",
      "Любовь Стрельникова\n",
      "Дмитрий Сутормин\n",
      "Алёна Сухопутова\n",
      "Алексей Тимошенко\n",
      "Александр Токарев\n",
      "Кристина Уласович\n",
      "Антон Ульяхин\n",
      "Елена Устинова\n",
      "Андрей Фельдман\n",
      "Ольга Филатова\n",
      "Александр Храмов\n",
      "Мария Шнырёва\n",
      "Илья Щеглов\n",
      "Динар Юнусов\n",
      "Александр Яровитчук\n",
      "Светлана Ястребова\n",
      "Сергей Ястребов\n"
     ]
    }
   ],
   "source": [
    "# Retrieve article texts and metadata for chosen authors\n",
    "\n",
    "data = []\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    with open('elementy_authors.json', 'r') as f:\n",
    "        js = json.load(f)\n",
    "        \n",
    "        for i in range(len(js[name])):\n",
    "            \n",
    "            d = {}\n",
    "\n",
    "            # ignore articles published prior to 2010\n",
    "            if int(js[name][i]['Publication date'][-4:]) < 2010: continue\n",
    "\n",
    "            url = js[name][i]['Article URL']\n",
    "            html = urlopen(url, context=ctx).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Metadata\n",
    "            meta = soup.find('div', class_='mb itemhead newslist')\n",
    "            title = meta.contents[0].get_text()\n",
    "            date = meta.find('span', class_='date').get_text()\n",
    "            topics = ', '.join([topic.get_text() for topic in meta('a')[1:-1]])\n",
    "\n",
    "            pattern = re.compile('Источник(и?)|См. также(:)?(\\s)?')\n",
    "            \n",
    "            # Full text\n",
    "            text = ''\n",
    "            tags = soup.find('div', class_='memo').contents\n",
    "            for tag in tags[:-2]: # ignore author's signature\n",
    "                # ignore illustrations, tables, etc.\n",
    "                if tag.name == 'p' or tag.name == 'ul':\n",
    "                    # ignore source materials\n",
    "                    if tag.get('class') and tag.get('class')[0] == 'small': continue\n",
    "                    if tag.find('b') and pattern.match(tag.find('b').get_text()): continue\n",
    "                    \n",
    "                    text += tag.get_text(' ', strip=True) + ' ' \n",
    "\n",
    "            if text:\n",
    "                d = {'Author': name, 'Title': title, 'Publication date': date, \\\n",
    "                    'Topics': topics, 'Article URL': url, 'Full text': text}\n",
    "                data.append(d)\n",
    "\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('elementy_texts.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a72475a4e957f639a8707c38a651116e9f027e6c6985c08c8cef46a94d0a95f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
