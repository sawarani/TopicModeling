{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9b41e7-a5bf-40e7-b4c5-56ffa647a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa674183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import functions as myf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9504d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['elementy_texts.json']\n",
    "\n",
    "df = myf.read_texts(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907ea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['Full text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910de688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unpreprocessed texts\n",
    "with open(f'raw_texts.json', 'w') as f:\n",
    "    json.dump(texts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b43a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenization with named entity recognition \n",
    "\n",
    "tokens = myf.get_named_ents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b72a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (Alt) Tokenization without named entity recognition \n",
    "\n",
    "tokens = myf.get_tokens(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df771600-40f0-4b45-b8bd-c639ad71f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove punctuation\n",
    "\n",
    "tokens_clean = myf.clean_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4b2496-4405-4c74-9076-2259780dbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lemmatize\n",
    "\n",
    "lemmas = myf.get_lemmas(tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "997350ce-a391-47d9-aa19-d8600599db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove lemmas with length 2 and less\n",
    "\n",
    "lemmas_no_short = [[word for word in text if len(word) > 2] for text in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c54630-f54c-4b1f-a33f-bd1f4c1e0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove stopwords\n",
    "\n",
    "lemmas_clean = myf.remove_stopwords(lemmas_no_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2577e4d2-e715-4eb6-8227-a38295a30197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Keep nouns and adjectives only\n",
    "\n",
    "nouns_adj = myf.get_nouns_adj(lemmas_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed90c114-593f-49a2-8b75-350f3dcb0fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Retrieve collocations/n-grams\n",
    "\n",
    "texts_ngrams = myf.get_ngrams(nouns_adj, min_count=200, threshold=100)\n",
    "\n",
    "# Save\n",
    "with open('data.json', 'w') as f:\n",
    "    data = [[word for word in text] for text in texts_ngrams]\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe32ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2289\n",
      "Corpus size:\n",
      "\tAfter tokenization: 3,888,311\n",
      "\tAfter lemmatization: 3,000,161\n",
      "\tAfter stopwords removal: 1,949,847\n",
      "\tAfter bad POS removal: 1,419,004\n",
      "\tAfter n-gram extraction: 1,415,022\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "\n",
    "print(f'Number of documents: {len(texts)}\\n' + 'Corpus size:')\n",
    "print(f'\\tAfter tokenization: {sum([len(doc) for doc in tokens]):,}' +\n",
    "      f'\\n\\tAfter lemmatization: {sum([len(doc) for doc in lemmas]):,}' +\n",
    "      f'\\n\\tAfter stopwords removal: {sum([len(doc) for doc in lemmas_clean]):,}' +\n",
    "      f'\\n\\tAfter bad POS removal: {sum([len(doc) for doc in nouns_adj]):,}'\n",
    "      f'\\n\\tAfter n-gram extraction: {sum([len(doc) for doc in texts_ngrams]):,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a72475a4e957f639a8707c38a651116e9f027e6c6985c08c8cef46a94d0a95f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
