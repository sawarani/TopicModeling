{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9b41e7-a5bf-40e7-b4c5-56ffa647a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa674183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9504d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # max num of texts per author\n",
    "\n",
    "texts = mf.read_texts(n)\n",
    "\n",
    "# Save\n",
    "with open(f'raw_texts.json', 'w') as f:\n",
    "    json.dump(texts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261c3657-5d90-446b-a16f-7041681f91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Evaluate the size of the corpus before preprocessing\n",
    "corpus_size = mf.get_corpus_size(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3eb3d5-42a9-49b8-b27d-a39c65c96c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Named entity recognition + tokenization\n",
    "\n",
    "texts_ne = mf.get_named_ents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df771600-40f0-4b45-b8bd-c639ad71f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove punctuation\n",
    "\n",
    "texts_clean = mf.clean_texts(texts_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4b2496-4405-4c74-9076-2259780dbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lemmatize\n",
    "\n",
    "lemmas = mf.get_lemmas(texts_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997350ce-a391-47d9-aa19-d8600599db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove lemmas with length 2 and less\n",
    "\n",
    "lemmas_no_short = [[word for word in text if len(word) > 2] \\\n",
    "                   for text in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c54630-f54c-4b1f-a33f-bd1f4c1e0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Remove stopwords\n",
    "\n",
    "lemmas_no_sw = mf.remove_stopwords(lemmas_no_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed90c114-593f-49a2-8b75-350f3dcb0fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Retrieve collocations/n-grams\n",
    "\n",
    "texts_ngrams = mf.get_ngrams(lemmas_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2577e4d2-e715-4eb6-8227-a38295a30197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Keep nouns and adjectives only\n",
    "\n",
    "nouns_adj = mf.get_nouns_adj(texts_ngrams)\n",
    "\n",
    "# Save\n",
    "with open(f'tokens.json', 'w') as f:\n",
    "    json.dump(nouns_adj, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19ef6bb-58ca-4ca4-ad91-05320faffab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2643\n",
      "\n",
      "Corpus size\n",
      "\tBefore preprocessing: 2,969,522\n",
      "\tAfter tokenization and NER: 3,670,204\n",
      "\tAfter lemmatization: 2,944,566\n",
      "\tAfter stopwords removal: 1,863,368\n",
      "\tAfter n-gram extraction: 1,797,966\n",
      "\tAfter bad POS removal (nouns and adj): 1,296,640\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "\n",
    "print(f'Number of documents: {len(texts)}')\n",
    "print(f'\\nCorpus size\\n\\tBefore preprocessing: {corpus_size:,}' +\n",
    "      f'\\n\\tAfter tokenization and NER: {sum([len(doc) for doc in texts_ne]):,}' +\n",
    "      f'\\n\\tAfter lemmatization: {sum([len(doc) for doc in lemmas]):,}' +\n",
    "      f'\\n\\tAfter stopwords removal: {sum([len(doc) for doc in lemmas_no_sw]):,}' +\n",
    "      f'\\n\\tAfter n-gram extraction: {sum([len(doc) for doc in texts_ngrams]):,}' +\n",
    "      f'\\n\\tAfter bad POS removal (nouns and adj): {sum([len(doc) for doc in nouns_adj]):,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a72475a4e957f639a8707c38a651116e9f027e6c6985c08c8cef46a94d0a95f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
